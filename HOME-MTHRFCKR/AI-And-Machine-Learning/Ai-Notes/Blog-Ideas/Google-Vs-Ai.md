# Google-vs-ai

published! https://twitter.com/swyx/status/1614278842356731910

---

Pro Google

- Google has insane AI expertise
	- you can try it right now
		- you dot com and neeva https://twitter.com/debarghya_das/status/1611555488776458242?s=46&t=fMwE5qHjZ29z2Apm-SG4mQ
	- Google BERT has been running billions of times a day
		- https://twitter.com/sterlingcrispin/status/1606064294663069696?s=20
		- replaced by MUM in 2021 https://towardsdatascience.com/rip-bert-googles-mum-is-coming-cb3becd9670f
		- nice diagram https://searchengineland.com/google-mum-update-seo-future-383551
	- Google PaLM (Pathways Language Model) in 2021
		- Pathways will enable us to train a single model to do thousands or millions of tasks.
			- We want a model to have different capabilities that can be called upon as needed, and stitched together to perform new, more complex tasks–a bit closer to the way the mammalian brain generalizes across tasks.
		- Pathways will enable multimodal models that encompass vision, auditory, and language understanding simultaneously.
			- And of course an AI model needn’t be restricted to these familiar senses; Pathways could handle more abstract forms of data, helping find useful patterns that have eluded human scientists in complex systems such as climate dynamics.
		- Pathways will make models sparse and efficient, only activating the relevant parts of the network for the given task.
			- For example, GShard and Switch Transformer are two of the largest machine learning models we’ve ever created, but because both use sparse activation, they [consume less than 1/10th the energy](https://blog.google/technology/ai/minimizing-carbon-footprint/) that you’d expect of similarly sized dense models—while being as accurate as dense models.
		- PaLM 540B beats GPT3 175B on all categories https://twitter.com/sterlingcrispin/status/1606309065730170880/photo/1
	- Google LaMDA
		- https://blog.google/technology/ai/lamda/
	- agreement from emad
		- https://twitter.com/EMostaque/status/1610609874743738370
- PageRank origins ([tweet](https://twitter.com/mmitchell_ai/status/1605013368560943105?s=20))
	- pre pagerank was gpt-retrieval like
	- With PageRank, the fact that websites link to one another could be used to identify which websites were *the most* linked to. The *most linked* sites are the ones people tend to want.
- link vs answer? [ben evans tweet](https://twitter.com/benedictevans/status/1607547804108431362)
	- When you were given a link did you really want an answer? Or did you, in fact, want a link?
	- I would like to see a matrix of query volume versus search ad revenue versus susceptibility to an answer-based response. The respond to "give me a personal injury lawyer" can still have ads even if it's delivered by GPT…

ai is feafure not product  
https://twitter.com/yacinemtb/status/1612997351659945986?s=46&t=J346-y4Gcs8C5beSeKYreA

openai was spending 70m with google cloud before msft investment https://twitter.com/amir/status/1590015635765219336

ai substack clone https://twitter.com/kantrowitz/status/1613924005064462336?s=46&t=d1UrpSjJB1LHnxU9IP9CZQ

hardware  
cerebras and graphcore
