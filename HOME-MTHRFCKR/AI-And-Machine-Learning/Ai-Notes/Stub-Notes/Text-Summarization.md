# Text-summarization

- [TL;DR: Mining Reddit to Learn Automatic Summarization, 2017](https://aclanthology.org/W17-4508.pdf)
	- After filtering, we are left with approximately 1.6 million submissions and 2.4 million comments for a total of 4 million content-summary pairs.
- https://github.com/yixinL7/BRIO
- LongT5 https://github.com/google-research/longt5
- Pegasus?
- Quillbot?
- gensim?
- BLOOM not as good https://huggingface.co/bigscience/bloom/discussions/122
- [Learning to summarize from human feedback, OpenAI 2020](https://proceedings.neurips.cc/paper/2020/file/1f89885d556929e98d3ef9b86448f951-Paper.pdf)
	- In this work, we show that it is possible to significantly improve summary quality by training a model to optimize for human preferences. We collect a large, high-quality dataset of human comparisons between summaries, train a model to predict the human-preferred summary, and use that model as a reward function to fine-tune a summarization policy using reinforcement learning. We apply our method to a version of the TL;DR dataset of Reddit posts [63] and find that our models significantly outperform both human reference summaries and much larger models fine-tuned with supervised learning alone.
- https://huggingface.co/datasets/openai/summarize_from_feedback
- This is the dataset of human feedback that was released for reward modelling. There are two parts of this dataset: `comparisons` and `axis`. In the `comparisons`part, human annotators were asked to choose the best out of two summaries. In the `axis` part, human annotators gave scores on a likert scale for the quality of a summary. The `comparisons` part only has a train and validation split, and the `axis` part only has a test and validation split. The summaries used for training the reward model in the paper come from the TL;DR dataset. Additional validation and test data come from the TL;DR dataset, CNN articles, and Daily Mail articles.
- ROUGEhttps://www.freecodecamp.org/news/what-is-rouge-and-how-it-works-for-evaluation-of-summaries-e059fb8ac840/
	- ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation
	- It works by comparing an **automatically produced summary** or **translation** against a set of **reference summaries** (typically human-produced).
	- ROUGE-N—measures **unigram**, **bigram**, **trigram** and higher order n-gram overlap
	- ROUGE-L—measures **longest matching sequence** of words using LCS. Since it automatically includes longest in-sequence common n-grams, you don’t need a predefined n-gram length.
	- ROUGE-S—Is any pair of words in a sentence in order, allowing for arbitrary gaps. This can also be called skip-gram concurrence.
	- it is always best to compute both the **precision** and **recall** and then report the **F-Measure**.
- SMMRY example https://www.reddit.com/r/MachineLearning/comments/wiqjxv/d_the_current_and_future_state_of_aiml_is/
