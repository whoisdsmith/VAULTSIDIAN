# An overview of gradient descent optimization algorithms

cover: https://ruder.io/content/images/2016/09/loss_function_image_tumblr.png
created: December 26, 2022 5:08 AM (UTC)
description: Gradient descent is the preferred way to optimize neural networks and many other machine learning algorithms but is often used as a black box. This post explores how many of the most popular gradient-based optimization algorithms such as Momentum, Adagrad, and Adam actually work.
folder: AI Systems / AI | Articles
tags: ai, articles
url: https://ruder.io/optimizing-gradient-descent