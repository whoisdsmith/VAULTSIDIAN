# MLOps Papers

---

This section lists scientific and industrial papers about ML operalization since 2015.

## 2022

1. **[Tiny-MLOps: a framework for orchestrating ML applications at the far edge of IoT systems](https://ieeexplore.ieee.org/document/9787703)** <br>
2. **[Machine Learning Operations (MLOps): Overview, Definition, and Architecture](https://arxiv.org/pdf/2205.02302.pdf)** <br>

## 2021

1. **[A software engineering perspective on engineering machine learning systems: State of the art and challenges](https://www.sciencedirect.com/science/article/pii/S016412122100128X)** <br>
A systematic analysis and summary of the current state of software engineering research for engineering ML systems.
2. **[Asset management in machine learning: a survey](https://arxiv.org/abs/2102.06919)** <br>
This paper presents a feature-based survey of 17 tools with ML asset management support identified in a systematic search. It overviews these tools’ features for managing the different types of assets used for engineering ML-based systems and performing experiments.
3. **[Ease.ML: a lifecycle management system for MLDev and MLOps](https://www.research-collection.ethz.ch/handle/20.500.11850/458916)** <br> This paper presents a system for managing and automating the entire lifecycle of machine learning application development.
4. **[Challenges in deploying machine learning: a survey of case studies](https://arxiv.org/abs/2011.09926)** <br> This survey reviews published reports of deploying machine learning solutions in a variety of use cases, industries and applications and extracts practical considerations corresponding to stages of the machine learning deployment workflow.
5. [Fischer, Lukas, Lisa Ehrlinger, Verena Geist, Rudolf Ramler, Florian Sobiezky, Werner Zellinger, David Brunner, Mohit Kumar, and Bernhard Moser. "AI System Engineering—Key Challenges and Lessons Learned."](https://www.mdpi.com/2504-4990/3/1/4)
6. [A Data Quality-Driven View of MLOps](https://arxiv.org/pdf/2102.07750.pdf)
7. [Towards Accountability for Machine Learning Datasets: Practices from Software Engineering and Infrastructure](https://arxiv.org/pdf/2010.13561.pdf)
8. [Production Machine Learning Pipelines: Empirical Analysis and Optimization Opportunities](https://arxiv.org/pdf/2103.16007.pdf)
9. [Muralidhar, Nikhil, et al. "Using AntiPatterns to avoid MLOps Mistakes." arXiv preprint arXiv:2107.00079 (2021).](https://arxiv.org/pdf/2107.00079)
10. [ModelCI-e: Enabling Continual Learning in Deep Learning Serving Systems](https://arxiv.org/abs/2106.03122) <br>
This paper implements a lightweight MLOps plugin, termed ModelCI-e (continuous integration and evolution). It embraces continual learning (CL) and ML deployment techniques, providing end-to-end supports for model updating and validation without serving engine customization.
11. [Hopkins, Aspen, and Serena Booth. "Machine Learning Practices Outside Big Tech: How Resource Constraints Challenge Responsible Development." (2021).](http://www.slbooth.com/papers/AIES-2021_Hopkins_and_Booth.pdf)

## 2020

1. **[Adoption and effects of software engineering best practices in machine learning](https://arxiv.org/pdf/2007.14130.pdf)** <br> This paper aims to empirically determine the state of the art in how teams develop, deploy and maintain software with ML components.
2. **[A viz recommendation system: ML lifecycle at Tableau](https://haikufactory.com/wp-content/uploads/2020/03/MLOps2020_Cerebro.pdf)** <br> This paper cover Tableau's research and development effort for the ML models behind the recommendation especially in the area of model life-cycle management, deployment, and monitoring.
3. **[Building continuous integration services for machine learning](http://pages.cs.wisc.edu/~wentaowu/papers/kdd20-ci-for-ml.pdf)** <br> This paper presents a CI system for ML that integrates seamlessly with existing ML development tools.
4. **[CodeReef: an open platform for portable MLOps, reusable automation actions and reproducible benchmarking](https://arxiv.org/pdf/2001.07935v1.pdf)** <br> This paper present CodeReef, an open source platform to share all the components necessary to enable cross-platform (MLSysOps), i.e., automating the deployment of ML models across diverse system in the most efficient way.
5. **[Common problems with creating machine learning pipelines from existing code](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/b50bc83882bbd29c50250d1e59fbc3afda3fb5e5.pdf)** This workshop paper shares common problems observed in industry on developing machine learning pipelines.
6. **[Data engineering for data analytics: a classification of the issues and case studies](https://arxiv.org/pdf/2004.12929.pdf)** <br> This paper provides a description and classification of data engineering tasks (such as acquiring, understanding, cleaning, and preparing the data) into high-levels groups, namely data organization, data quality, and feature engineering.
7. **[DevOps for AI - challenges in development of AI-enabled applications](https://ieeexplore.ieee.org/abstract/document/9238323?casa_token=f5wqYYVhaiQAAAAA:rSCh5ui6KzXtcn81pPVqKsEM4GX5LYmMBjIRljD0NvQEcekH1NXBYZRFPfBQAextDTv9MI0Hpg)** <br> This paper points out the challenges in development of complex systems that include ML components, and discuss possible solutions driven by the combination of DevOps and ML workflow processes. Industrial cases are presented to illustrate these challenges and the possible solutions.
8. **[Developments in MLflow: a system to accelerate the machine learning lifecycle](https://dl.acm.org/doi/pdf/10.1145/3399579.3399867)** <br> This paper discusses user feedback collected since MLflow was launched in 2018, as well as three major features introduced in response to this feedback.
9. **[Engineering AI systems: a research agenda](https://arxiv.org/pdf/2001.07522.pdf)** <br> This paper presents a research agenda for AI engineering that provides an overview of the key engineering challenges surrounding ML solutions and an overview of open items that need to be addressed by the research community at large.
10. **[Explainable machine learning in deployment](https://dl.acm.org/doi/pdf/10.1145/3351095.3375624)** <br> This study explores how organizations view and use explainability for stakeholder consumption.
11. **[From what to how: an initial review of publicly available AI ethics tools, methods and research to translate principles into practices](https://link.springer.com/article/10.1007/s11948-019-00165-5)** <br> This papers aims at contributing to closing the gap between principles and practices in Machine Learning by constructing a typology that may help practically-minded developers apply ethics at each stage of the Machine Learning development pipeline, and to signal to researchers where further work is needed.
12. **[Implicit provenance for machine learning artifacts](https://dcatkth.github.io/papers/provenance_mlsys20.pdf)** <br> This paper presents an approach, called implicit provenance, where a distributed file system  
and APIs are instrumented to capture changes to ML artifacts, that, along with file naming conventions, mean that full lineage can be tracked for TensorFlow/Keras/Pytorch programs without requiring code changes.
13. **[Machine learning testing: survey, landscapes and horizons](https://ieeexplore.ieee.org/abstract/document/9000651?casa_token=r7y3n1BRlCEAAAAA:nQvqwkRoNBwigFGaBjusBly0XU_k2hPkLA8ob6GJS392_U6S8QhR-nsuyAgFeF0CrOsT-L5dCQ)** <br> This paper provides a comprehensive survey of Machine Learning Testing (ML testing) research.
14. **[MLModelCI: an automatic cloud platform for efficient MLaaS](https://dl.acm.org/doi/abs/10.1145/3394171.3414535)** <br> This paper presents MLModelCI, a one-step platform for efficient machine learning (ML) services that leverages DevOps techniques to optimize, test, and manage models. It also containerizes and deploys these optimized and validated models as cloud services.
15. **[Monitoring and explainability of models in production](https://arxiv.org/pdf/2007.06299.pdf)** <br> This paper discusses the challenges to successful implementation of solutions in key areas (such as model performance and data monitoring, detecting outliers and data drift using statistical techniques) with some recent examples of production ready solutions using open source tools.
16. **[Principles and practice of explainable machine learning](https://arxiv.org/pdf/2009.11698.pdf)** <br> This paper focuses on data-driven methods - machine learning and pattern recognition models in particular - so as to survey and distill the results and observations from the literature about the following challenges: *how do we understand the decisions suggested by these systems in order that we can trust them?*
17. **[sensAI: fast ConvNets serving on live data via class parallelism](https://rise.cs.berkeley.edu/wp-content/uploads/2020/01/sensAI_2_pager.pdf)** <br> This paper presents sensAI, a novel and generic approach to achieve faster inference on single data item, that distributes a single CNN into disconnected subnets, and achieve decent serving accuracy with negligible communication overhead (1 float value).
18. **[Software engineering for artificial intelligence and machine learning software: a systematic literature review](https://arxiv.org/pdf/2011.03751v1.pdf)** <br> This study aims to investigate how software engineering (SE) has been applied in the development of AI/ML systems and identify challenges and practices that are applicable and determine whether they meet the needs of professionals.
19. **Software engineering patterns for machine learning applications (SEP4MLA)** <br> From 33 ML patterns, this paper describes three major ML architecture patterns and one ML design pattern in the standard pattern format so that practitioners can (re)use them in their contexts. [Go to part 1](https://pl.csie.ntut.edu.tw/asianplop2020/papers/AsianPLoP_2020_paper_13.pdf) [or part 2](https://hillside.net/plop/2020/papers/washizaki.pdf)
20. **[Simulating performance of ML systems with offline profiling](https://arxiv.org/pdf/2002.06790.pdf)** <br> This paper advocates that simulation based on offline profiling is a promising approach to better understand and improve the complex ML systems, and proposes and approach that uses operation-level profiling and dataflow based simulation to ensure a unified and automated solution for all frameworks and ML models.
21. **[Towards automating the AI operations lifecycle](https://arxiv.org/pdf/2003.12808)** <br> This paper presents a set of enabling technologies that can be used to increase the level of automation in AI operations, thus lowering the human effort required.
22. **[Towards CRISP-ML(Q): a machine learning process model with quality assurance methodology](https://arxiv.org/pdf/2003.05155.pdf)** <br> This paper proposes a process model for the development of machine learning applications that guides machine learning practitioners and project organizations from industry and academia with a checklist of tasks that spans the complete project life-cycle.
23. **[Towards distribution transparency for supervised ML with oblivious training functions](https://content.logicalclocks.com/hubfs/research/oblivious-training_mlsys20.pdf)** <br> This paper introduces the distribution oblivious training function as an abstraction for ML development in Python, whereby developers can reuse the same training function when running a notebook on a laptop or performing scale-out hyperparameter search and distributed training on clusters.
24. **[Towards ML engineering: a brief history of TensorFlow Extended (TFX)](https://arxiv.org/pdf/2010.02013.pdf)** <br> This paper gives a whirlwind tour of Sibyl and TensorFlow Extended (TFX), two successive end-to-end ML platforms at Alphabet. It also shares the lessons learned from over a decade of applied ML built on these platforms, and explains both their similarities and their differences.
25. [Siebert, Julien, et al. "Towards guidelines for assessing qualities of machine learning systems." International Conference on the Quality of Information and Communications Technology. Springer, Cham, 2020.](https://arxiv.org/pdf/2008.11007)
26. [Karlaš, Bojan, Matteo Interlandi, Cedric Renggli, Wentao Wu, Ce Zhang, Deepak Mukunthu Iyappan Babu, Jordan Edwards, Chris Lauren, Andy Xu, and Markus Weimer. "Building continuous integration services for machine learning." In Proceedings of the 26th ACM SIGKDD 2020.](https://www.researchgate.net/profile/Wentao-Wu-2/publication/343776671_Building_Continuous_Integration_Services_for_Machine_Learning/links/5fb6d4c8299bf104cf5cd3b1/Building-Continuous-Integration-Services-for-Machine-Learning.pdf)

## 2019

1. **[Assuring the machine learning lifecycle: desiderata, methods, and challenges](https://arxiv.org/pdf/1905.04223.pdf)** <br> This paper provides a comprehensive survey of the state-of-the-art in the assurance of ML, i.e., in the generation of evidence that ML is sufficiently safe for its intended use.
2. **[Continuous integration of machine learning models with ease.ml/ci: towards a rigorous yet practical treatment](https://arxiv.org/abs/1903.00278)** <br> This paper presents ease.ml/ci, a continuous integration system for machine learning to provide rigorous guarantees with a practical amount of labeling effort.
3. **[Challenges in the deployment and operation of machine learning in practice](https://aisel.aisnet.org/ecis2019_rp/163/)** <br> In this work, the authors target to systematically elicit the challenges in deployment and operation to enable broader practical dissemination of machine learning applications.
4. **[Overton: a data system for monitoring and improving machine-learned products](https://www.cs.stanford.edu/~chrismre/papers/overton-tr.pdf)** <br> This paper describes a system called Overton, whose main design goal is to support engineers in building, monitoring, and improving production machine learning systems.
5. **[Studying software engineering patterns for designing machine learning systems](https://ieeexplore.ieee.org/abstract/document/8945075?casa_token=f5hH9ZiTf6oAAAAA:MVO9k_wRuFLbi0MnCTVJ1p2A5We_paqfJ9v7nAG0NR5swI8GJTGA4rxYykzQ7TwfP3BiLbXJNQ)** <br> This paper collects good/bad software engineering design patterns for ML techniques to provide developers with a comprehensive classification of such patterns.
6. **[Towards automated ML model monitoring: measure, improve and quantify data quality](https://ssc.io/pdf/autoops.pdf)** <br> This paper focuses on the arising challenge of automating the operation of deployed ML applications, especially with respect to monitoring the quality of their input data.

## 2018

1. **[A systems perspective to reproducibility in production machine learning domain](https://openreview.net/pdf?id=Byl4vavigX)** This paper presents a system that enables ML experts to track and reproduce ML models and pipelines in production.
2. **[Building a reproducible machine learning pipeline](https://arxiv.org/ftp/arxiv/papers/1810/1810.04570.pdf)** This paper discusses some problems encountered while building a variety of machine learning models, and subsequently describes a framework to tackle the problem of model reproducibility.
3. **[On challenges in machine learning model management](https://web.kaust.edu.sa/Faculty/MarcoCanini/classes/CS290E/F19/papers/challenges.pdf)** <br> This paper discusses a selection of ML use cases, develops an overview over conceptual, engineering, and data-processing related challenges arising in the management of the  
corresponding ML models, and points out future research directions.
4. **[Ease.ml in action: towards multi-tenant declarative learning services](http://www.vldb.org/pvldb/vol11/p2054-karlas.pdf)** <br> This demo paper presents the design principles of ease.ml, highlights the implementation of its key components, and showcases how ease.ml can help ease machine learning tasks that often perplex even experienced users.

## 2017

1. **[Clipper: a low-latency online prediction serving system](https://www.usenix.org/conference/nsdi17/technical-sessions/presentation/crankshaw)** <br> This paper introduces Clipper, a general-purpose low-latency prediction serving system that aims to simplify model deployment across frameworks and applications, reduce prediction latency, and improve prediction throughput, accuracy, and robustness without modifying the underlying machine learning frameworks.
2. **[Ease.ml: towards multi-tenant resource sharing for machine learning workloads](https://arxiv.org/abs/1708.07308)** <br> This paper presents ease.ml, a declarative machine learning service platform.
3. **[Data management challenges in production machine learning](https://dl.acm.org/doi/10.1145/3035918.3054782)** <br> This paper discusses data-management issues that arise in the context of machine learning pipelines deployed in production.
4. **[TFX: A TensorFlow-based production-scale machine learning platform](https://dl.acm.org/doi/10.1145/3097983.3098021)** <br> This paper presents TensorFlow Extended (TFX), a TensorFlow-based general-purpose machine learning platform implemented at Google to reduce the time to production from the order of months to weeks, while providing platform stability that minimizes disruptions.

## 2016

1. **[ModelDB: a system for machine learning model management](https://www-cs.stanford.edu/~matei/papers/2016/hilda_modeldb.pdf)** <br> This paper describes ModelDB, a novel end-to-end system for the management of machine learning models.
2. **[Scaling Machine Learning as a Service](http://proceedings.mlr.press/v67/li17a/li17a.pdf)** <br> This paper presents the scalable MLaaS built for Uber that operates globally. It focus on several challenges, among which: (i) how to scale feature computation for many machine learning use cases; (ii) how to build accurate models using global data; (iii) how to enable scalable model deployment and real-time serving for many models across multiple data centers.
3. **[What’s your ML test score? A rubric for ML production systems](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/45742.pdf)** <br> This paper presents an ML Test Score rubric based on a set of actionable tests to help quantify a host of issues not found in small toy examples or even large offline research experiments.

## 2015

1. **[Hidden technical debt in machine learning systems](https://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf)** <br> This paper explores several ML-specific risk factors to account for in system design.

## Additional Resources

1. [Adversarial machine learning reading list](https://nicholas.carlini.com/writing/2018/adversarial-machine-learning-reading-list.html)
2. [Workshop at ICML 2020: "Challenges in Deploying and Monitoring Machine Learning Systems" (Accepted Papers)](https://icml.cc/Conferences/2020/Schedule?showEvent=5738)
3. [Workshop on MLOps Systems (MLSys)](https://mlops-systems.github.io/)
4. [A survey on concept drift adaptation](http://eprints.bournemouth.ac.uk/22491/1/ACM%20computing%20surveys.pdf)
5. [Beyond Accuracy: Behavioral Testing of NLP Models with CheckList](https://arxiv.org/pdf/2005.04118.pdf)
6. *Conversational Applications and Natural Language Understanding Services at Scale.* Minh Tue Vo Thanh and Vijay Ramakrishnan.
7. *Efficient Scheduling of DNN Training on Multitenant Clusters.* Deepak Narayanan, Keshav Santhanam, Amar Phanishayee and Matei Zaharia.
8. *MLBox: Towards Reproducible ML.* Victor Bittorf, Xinyuan Huang, Peter Mattson, Debojyoti Dutta, David Aronchick, Emad Barsoum, Sarah Bird, Sergey Serebryakov, Natalia Vassilieva, Tom St. John, Grigori Fursin, Srini Bala, Sivanagaraju Yarramaneni, Alka Roy, David Kanter and Elvira Dzhuraeva.
9. *MLPM: Machine Learning Package Manager.* Xiaozhe Yao.
10. *Tools for machine learning experiment management.* Vlad Velici and Adam Prügel-Bennett.
11. *Towards split learning at scale: System design.* Iker Rodríguez, Eduardo Muñagorri, Alberto Roman, Abhishek Singh, Praneeth Vepakomma and Ramesh Raskar.
12. [(2020) Towards complaint-driven ML workflow debugging.](http://wooya.me/files/MLSys_2020.pdf)
13. [(NA) PerfGuard: Deploying ML-for-Systems without Performance Regressions.](http://jindal-web.appspot.com/papers/perfguard.pdf)
14. [Addressing the Memory Bottleneck in AI Model-Training](https://arxiv.org/pdf/2003.08732)
15. [Reliance on Metrics is a Fundamental Challenge for AI](https://arxiv.org/pdf/2002.08512.pdf)
16. [Teaching Software Engineering for AI-Enabled Systems](https://arxiv.org/pdf/2001.06691.pdf)

---

#AI #awesome
