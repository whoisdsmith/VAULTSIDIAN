{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davideriboli/Deep-Art/blob/main/02_Music/Yoda/Yoda.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "gradient": {
          "editing": false,
          "id": "ac5a4cf0-d9d2-47b5-9633-b53f8d99a4d2",
          "kernelId": ""
        },
        "id": "SiTIpPjArIyr"
      },
      "source": [
        "# Yoda (ver. 1.0)\n",
        "\n",
        "***\n",
        "\n",
        "Powered by tegridy-tools TMIDIX Optimus Processors: https://github.com/asigalov61/tegridy-tools\n",
        "\n",
        "***\n",
        "\n",
        "Credit for GPT2-RGA code used in this colab goes out @ Sashmark97 https://github.com/Sashmark97/midigen and @ Damon Gwinn https://github.com/gwinndr/MusicTransformer-Pytorch\n",
        "\n",
        "***\n",
        "\n",
        "WARNING: This complete implementation is a functioning model of the Artificial Intelligence. Please excercise great humility, care, and respect. https://www.nscai.gov/\n",
        "\n",
        "***\n",
        "\n",
        "#### Project Los Angeles\n",
        "\n",
        "#### Tegridy Code 2022\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "gradient": {
          "editing": false,
          "id": "fa0a611c-1803-42ae-bdf6-a49b5a4e781b",
          "kernelId": ""
        },
        "id": "gOd93yV0sGd2"
      },
      "source": [
        "# (Setup Environment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "gradient": {
          "editing": false,
          "id": "39411b40-9e39-416e-8fe4-d40f733e7956",
          "kernelId": ""
        },
        "id": "lw-4aqV3sKQG"
      },
      "outputs": [],
      "source": [
        "#@title nvidia-smi gpu check\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "gradient": {
          "editing": false,
          "id": "a1a45a91-d909-4fd4-b67a-5e16b971d179",
          "kernelId": ""
        },
        "id": "fX12Yquyuihc"
      },
      "outputs": [],
      "source": [
        "#@title Install all dependencies (run only once per session)\n",
        "\n",
        "!git clone https://github.com/asigalov61/Yoda\n",
        "!pip install torch\n",
        "!pip install tqdm\n",
        "!pip install matplotlib\n",
        "\n",
        "!apt install fluidsynth #Pip does not work for some reason. Only apt works\n",
        "!pip install midi2audio\n",
        "!pip install pretty_midi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "gradient": {
          "editing": false,
          "id": "b8207b76-9514-4c07-95db-95a4742e52c5",
          "kernelId": ""
        },
        "id": "z7n9vnKmug1J"
      },
      "outputs": [],
      "source": [
        "#@title Import all needed modules\n",
        "\n",
        "print('Loading needed modules. Please wait...')\n",
        "import os\n",
        "import copy\n",
        "import tqdm as tqdm\n",
        "import random\n",
        "\n",
        "print('Loading TMIDIX module...')\n",
        "os.chdir('/content/Yoda')\n",
        "import TMIDIX\n",
        "from GPT2RGAX import *\n",
        "\n",
        "print('Loading aux modules...')\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from midi2audio import FluidSynth\n",
        "import pretty_midi\n",
        "import librosa.display\n",
        "from IPython.display import display, Javascript, HTML, Audio\n",
        "\n",
        "os.chdir('/content/')\n",
        "print('Done! Enjoy! :)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObPxlEutsQBj"
      },
      "source": [
        "# (MODEL)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Unzip pre-trained Yoda model and the training data file\n",
        "%cd /content/\n",
        "\n",
        "print('=' * 70)\n",
        "print('Unzipping pre-trained dataset-model...Please wait...')\n",
        "print('=' * 70)\n",
        "\n",
        "!cat /content/Yoda/Model/Yoda-Trained-Model.zip* > Yoda-Trained-Model.zip\n",
        "print('=' * 70)\n",
        "\n",
        "!unzip -j Yoda-Trained-Model.zip\n",
        "print('=' * 70)\n",
        "\n",
        "print('Done! Enjoy! :)')\n",
        "print('=' * 70)\n",
        "%cd /content/"
      ],
      "metadata": {
        "cellView": "form",
        "id": "q2YTwObSiwbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdKFoeke9L7H"
      },
      "source": [
        "# (LOAD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "gradient": {
          "id": "c83edd89-9a36-430a-9fa7-3a967417c88e",
          "kernelId": ""
        },
        "id": "OaNkGcFo9UP_"
      },
      "outputs": [],
      "source": [
        "#@title Load/Reload the model\n",
        "\n",
        "from collections import OrderedDict\n",
        "\n",
        "full_path_to_model_checkpoint = \"/content/Yoda-Trained-Model.pth\" #@param {type:\"string\"}\n",
        "\n",
        "print('Loading the model...')\n",
        "config = GPTConfig(21938, \n",
        "                   1024,\n",
        "                   dim_feedforward=512,\n",
        "                   n_layer=8, \n",
        "                   n_head=8, \n",
        "                   n_embd=512,\n",
        "                   enable_rpr=True,\n",
        "                   er_len=1024)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = GPT(config)\n",
        "\n",
        "state_dict = torch.load(full_path_to_model_checkpoint, map_location=device)\n",
        "\n",
        "new_state_dict = OrderedDict()\n",
        "for k, v in state_dict.items():\n",
        "    name = k[7:] #remove 'module'\n",
        "    new_state_dict[name] = v\n",
        "\n",
        "model.load_state_dict(new_state_dict)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "print('Done!')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load and prep the original training data which will be used to prime the model\n",
        "full_path_to_original_training_data = \"/content/Yoda_Training_Data\" #@param {type:\"string\"}\n",
        "\n",
        "melody_chords_f = TMIDIX.Tegridy_Any_Pickle_File_Reader(full_path_to_original_training_data)\n",
        "\n",
        "randomize_dataset = True\n",
        "\n",
        "print('=' * 70)\n",
        "print('Prepping INTs dataset...')\n",
        "\n",
        "if randomize_dataset:\n",
        "    print('=' * 70)\n",
        "    print('Randomizing the dataset...')\n",
        "    random.shuffle(melody_chords_f)\n",
        "    print('Done!')\n",
        "    \n",
        "print('=' * 70)\n",
        "print('Processing the dataset...')\n",
        "\n",
        "r = 0\n",
        "\n",
        "train_data1 = []\n",
        "\n",
        "itimes = []\n",
        "ipitches = []\n",
        "\n",
        "\n",
        "for chords_list in tqdm(melody_chords_f):\n",
        "    \n",
        "    train_data1.extend([0]) # Intro/Zero Token\n",
        "    \n",
        "    for i in chords_list:\n",
        "\n",
        "        if i[0] != 0: # This is the chordification line\n",
        "            train_data1.extend([i[0]]) # start-times\n",
        "            itimes.extend([i[0]])\n",
        "            ipitches.extend([i[1] + (i[2] * 16) + (i[3] * 16 * 128)])\n",
        "        # And this is the main MIDI note line (triple stack)\n",
        "        main_note = [i[1] + (i[2] * 16) + (i[3] * 16 * 128)] # Main note == [duration / pitch / channel]\n",
        "        \n",
        "        if main_note != [0]: # Main note error control...\n",
        "            train_data1.extend(main_note) # Main note == [duration / pitch / channel]\n",
        "\n",
        "print('Done!')        \n",
        "print('=' * 70)\n",
        "        \n",
        "print('Total INTs:', len(train_data1))\n",
        "print('Minimum INT:', min(train_data1))\n",
        "print('Maximum INT:', max(train_data1))\n",
        "print('Unique INTs:', len(set(train_data1)))\n",
        "print('Intro/Zero INTs:', train_data1.count(0))\n",
        "print('=' * 70)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "awnEp4NAh3Bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UX1_5y5Fu8AH"
      },
      "source": [
        "# (GENERATE MUSIC)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NOTE: Due to the size of the model and the dataset, the output will be (at best) a remix of the original compositions. Only large-scale models trained on huge datasets (like MuseNet) can produce original compositions."
      ],
      "metadata": {
        "id": "Low-mjthPyxS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "gradient": {
          "id": "97793d01-6a74-4e34-be95-ea337277b38d",
          "kernelId": ""
        },
        "id": "M_K93hWWv2Yx"
      },
      "outputs": [],
      "source": [
        "#@title Generate music\n",
        "\n",
        "#@markdown NOTE: Play with the settings to get different results\n",
        "\n",
        "\n",
        "priming_type = \"Random Intro\" #@param [\"Random Intro\", \"Random Point\"]\n",
        "freeze_priming_point = False #@param {type:\"boolean\"}\n",
        "number_of_instruments = 11 # DO NOT REDUCE for optimal performance !!!\n",
        "number_of_prime_tokens = 32 #@param {type:\"slider\", min:4, max:32, step:4}\n",
        "number_of_tokens_to_generate = 64 #@param {type:\"slider\", min:64, max:128, step:16}\n",
        "number_of_continuation_blocks = 40 #@param {type:\"slider\", min:10, max:100, step:5}\n",
        "temperature = 0.8 #@param {type:\"slider\", min:0.1, max:1, step:0.1}\n",
        "show_stats = False #@param {type:\"boolean\"}\n",
        "\n",
        "#===================================================================\n",
        "print('=' * 70)\n",
        "print('Yoda Music Model Continuation Generator')\n",
        "print('=' * 70)\n",
        "\n",
        "print('Generation settings:')\n",
        "print('=' * 70)\n",
        "print('Priming type:', priming_type)\n",
        "print('Number of instruments:', number_of_instruments)\n",
        "print('Number of prime tokens:', number_of_prime_tokens)\n",
        "print('Number of tokens:', number_of_tokens_to_generate)\n",
        "print('Number of continuation blocks:', number_of_continuation_blocks)\n",
        "print('Model temperature:', temperature)\n",
        "\n",
        "print('=' * 70)\n",
        "print('Prepping...')\n",
        "\n",
        "if not freeze_priming_point:\n",
        "  r = random.randint(0, len(train_data1))\n",
        "\n",
        "if priming_type == 'Random Intro':\n",
        "  idx = r+train_data1[r:].index(0)\n",
        "  out = train_data1[idx:idx+number_of_prime_tokens]\n",
        "else:\n",
        "  out = train_data1[r:r+number_of_prime_tokens]\n",
        "\n",
        "out1 = []\n",
        "\n",
        "tokens_range = (128 * 16 * number_of_instruments)\n",
        "\n",
        "print('=' * 70)\n",
        "print('Generating...')\n",
        "\n",
        "for i in range(number_of_continuation_blocks):\n",
        "\n",
        "  if show_stats: \n",
        "    print('=' * 70)\n",
        "    print('Block #', i)\n",
        "\n",
        "  rand_seq = model.generate(torch.Tensor(out[-number_of_prime_tokens:]), \n",
        "                                          target_seq_length=number_of_tokens_to_generate,\n",
        "                                          temperature=temperature,\n",
        "                                          stop_token=tokens_range,\n",
        "                                          verbose=show_stats)\n",
        "  \n",
        "  out = rand_seq[0].cpu().numpy().tolist()\n",
        "  \n",
        "  out1.extend(out[number_of_prime_tokens:])\n",
        "\n",
        "print('=' * 70)\n",
        "print('Done!')\n",
        "\n",
        "if show_stats:\n",
        "  print('=' * 70)\n",
        "  print('Detokenizing output...')\n",
        "\n",
        "if len(out1) != 0:\n",
        "    song = []\n",
        "    song = out1\n",
        "    song_f = []\n",
        "    time = 0\n",
        "    dur = 0\n",
        "    vel = 0\n",
        "    pitch = 0\n",
        "    channel = 0\n",
        "    \n",
        "    for s in song:\n",
        "        if s < 256:\n",
        "            time += s * 16\n",
        "            \n",
        "        else:\n",
        "            channel = s // 16 // 128\n",
        "\n",
        "            pitch = (s // 16) % 128\n",
        "            \n",
        "            dur = ((s % 16) * 128) + 128\n",
        "            \n",
        "            # Velocities for each channel:\n",
        "            if channel == 0:  # Piano     \n",
        "                vel = 60\n",
        "            if channel == 1:  # Guitar     \n",
        "                vel = 70            \n",
        "            if channel == 2:  # Bass     \n",
        "                vel = 60            \n",
        "            if channel == 3:  # Violin\n",
        "                vel = 90            \n",
        "            if channel == 4:  # Cello     \n",
        "                vel = 100\n",
        "            if channel == 5:  # Harp     \n",
        "                vel = 80\n",
        "            if channel == 6:  # Trumpet     \n",
        "                vel = 100            \n",
        "            if channel == 7:  # Clarinet     \n",
        "                vel = 100           \n",
        "            if channel == 8:  # Flute\n",
        "                vel = 100                          \n",
        "            if channel == 9:  # Drums\n",
        "                vel = 80            \n",
        "            if channel == 10:  # Choir     \n",
        "                vel = 110                  \n",
        "                               \n",
        "            song_f.append(['note', time, dur, channel, pitch, vel ])\n",
        "    \n",
        "    if show_stats:\n",
        "      print('=' * 70) # Converting to MIDI\n",
        "      \n",
        "    detailed_stats = TMIDIX.Tegridy_SONG_to_MIDI_Converter(song_f,\n",
        "                                                        output_signature = 'Yoda',  \n",
        "                                                        output_file_name = '/content/Yoda-Music-Composition', \n",
        "                                                        track_name='Project Los Angeles',\n",
        "                                                        list_of_MIDI_patches=[0, 24, 32, 40, 42, 46, 56, 71, 73, 0, 53, 0, 0, 0, 0, 0],\n",
        "                                                        number_of_ticks_per_quarter=500)\n",
        "\n",
        "    if show_stats:\n",
        "      print('=' * 70)\n",
        "      print('Detailed MIDI stats:')\n",
        "      for key, value in detailed_stats.items():\n",
        "            print('=' * 70)\n",
        "            print(key, '|', value)\n",
        "\n",
        "else:\n",
        "  print('Models output is empty! Check the code...')\n",
        "  print('Shutting down...')\n",
        "\n",
        "\n",
        "print('=' * 70)\n",
        "print('Displaying resulting composition...')\n",
        "fname = 'Yoda-Music-Composition'\n",
        "\n",
        "pm = pretty_midi.PrettyMIDI(fname + '.mid')\n",
        "\n",
        "# Retrieve piano roll of the MIDI file\n",
        "piano_roll = pm.get_piano_roll()\n",
        "\n",
        "plt.figure(figsize=(14, 5))\n",
        "librosa.display.specshow(piano_roll, x_axis='time', y_axis='cqt_note', fmin=1, hop_length=160, sr=16000, cmap=plt.cm.hot)\n",
        "plt.title(fname)\n",
        "\n",
        "FluidSynth(\"/usr/share/sounds/sf2/FluidR3_GM.sf2\", 16000).midi_to_audio(str(fname + '.mid'), str(fname + '.wav'))\n",
        "Audio(str(fname + '.wav'), rate=16000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzCMd94Tu_gz"
      },
      "source": [
        "# Congrats! You did it! :)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Yoda.ipynb",
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}