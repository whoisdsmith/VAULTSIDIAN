# benchmarks

text

- Answering the call, more than 130 institutions collaborated on [BIG-bench](https://www.deeplearning.ai/the-batch/toward-next-gen-language-models/), which includes tasks like deducing a movie title from emojis, participating in mock trials, and detecting logical fallacies.
- https://cs.nyu.edu/~davise/Benchmarks/
	- [Text](https://cs.nyu.edu/~davise/Benchmarks/Text.html)
	- [BIG-bench](https://cs.nyu.edu/~davise/Benchmarks/BIG-bench.html)
	- [Images](https://cs.nyu.edu/~davise/Benchmarks/Images.html)
	- [Videos](https://cs.nyu.edu/~davise/Benchmarks/Videos.html)
	- [Simulated Physical Worlds](https://cs.nyu.edu/~davise/Benchmarks/Physical.html)
	- [Symbolic/Knowledge Graphs](https://cs.nyu.edu/~davise/Benchmarks/Symbolic.html)
- riley goodside benchmark questions
	- https://scale.com/blog/chatgpt-vs-claude#Calculation
	- Overall, Claude is a serious competitor to ChatGPT, with improvements in many areas. While conceived as a demonstration of “constitutional” principles, Claude feels not only safer but more fun than ChatGPT. Claude’s writing is more verbose, but also more naturalistic. Its ability to write coherently about itself, its limitations, and its goals seem to also allow it to more naturally answer questions on other subjects.

HELM by Stanformd CRFM

- https://twitter.com/nathanbenaich/status/1610385056618663936?s=20&t=fBOWt8NvTwGGnwJ92tybAQ
- https://crfm.stanford.edu/helm/v0.2.0/?group=core_scenarios
- evaluates 34 prominent language models in a standardized way on 42 scenarios x 7 metrics.
