# How does GPT-3 work?

Files & media: 1.png

# How does GPT-3 work?

GPT-3 is a language prediction model. This means that it has a neural network machine learning model that can take input text and transform it into what it predicts the most useful result will be. This is accomplished by training the system on the vast body of internet text to spot patterns in a process called generative pre-training. GPT-3 was trained on several data sets with different weights, including Common Crawl, WebText2 and Wikipedia.

GPT-3 is first trained through a supervised testing phase and then a reinforcement phase. When training ChatGPT, a team of trainers ask the language model a question with a correct output in mind. If the model answers incorrectly, the trainers tweak it to teach it the right answer. The model may also give several answers, which trainers rank from best to worst.

GPT-3 has more than 175 billion machine learning parameters and is significantly larger than its predecessors -- previous large language models, such as Bidirectional Encoder Representations from Transformers (BERT) and Turing NLG. Parameters are the parts of a large language model that define its skill on a problem, such as generating text. Large language model performance generally scales as more data and parameters are added to the model.

When a user provides text input, the system analyzes the language and uses a text predictor based on training to create the most likely output. The model can be fine-tuned, but without much additional tuning or training, the model generates high-quality output text that feels similar to what humans would produce.

Explain to a 10-year-old:

<aside>
ðŸ’¡ When you give GPT-3 some words or sentences, it uses its super smart brain to figure out what you want it to say next. It's like it's trying to guess what you want to say. It's like playing a game of "what comes next?". It uses what it has learned before to help it guess the right words and phrases. Even if you don't give it much extra information, it can still write sentences that sound like a real person wrote. But if you give it a little more information, it can get even better at guessing what you want it to say. It's like giving your super smart friend more information to help them write a story for you.

</aside>

[https://twitter.com/JayAlammar/status/1285498971960598529?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1285989408856915968%7Ctwgr%5Eef3d70cd52c2cda06f4c77d8171c23b9e91456e5%7Ctwcon%5Es3_&ref_url=https%3A%2F%2Fwww.notion.so%2Fbuildbystl%2FIdeate-Build-your-first-GPT-3-powered-product-0bd797c989d749bbac7584b6fc9ab3c7%3Fp%3D69a8390cdee64b429d6c71dc1f13f4e9pm%3Dc](https://twitter.com/JayAlammar/status/1285498971960598529?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1285989408856915968%7Ctwgr%5Eef3d70cd52c2cda06f4c77d8171c23b9e91456e5%7Ctwcon%5Es3_&ref_url=https%3A%2F%2Fwww.notion.so%2Fbuildbystl%2FIdeate-Build-your-first-GPT-3-powered-product-0bd797c989d749bbac7584b6fc9ab3c7%3Fp%3D69a8390cdee64b429d6c71dc1f13f4e9pm%3Dc)

The massive amount of data used to train GPT-3 enables the model to understand and generate human-like text in several ways:

- Vocabulary: The large amount of text data used to train GPT-3 allows the model to learn a vast vocabulary of words, phrases, and idioms. This enables the model to understand and generate text closer to human-like text.
- Language structure: By training on such a large dataset of text, GPT-3 learns the structure and patterns of human language. This allows the model to understand and generate grammatically correct and coherent text.
- Contextual understanding: The large amount of text data used to train GPT-3 allows the model to learn the context in which words and phrases are used. This enables the model to understand and generate appropriate text for the task or context.
- Domain-specific knowledge: GPT-3's training data includes a diverse range of text from various domains such as science, technology, art, and politics. This allows the model to acquire domain-specific knowledge and generate appropriate text for a specific domain.
- Fine-tuning: GPT-3's pre-training on such a large dataset allows for fine-tuning the model on a specific task or domain. This fine-tuning enables GPT-3 to generate text specific to the task or domain, making it more effective for certain applications.